{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\users\\jorge\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy\n",
    "\n",
    "def openFile(root):\n",
    "    corpus = PlaintextCorpusReader(root, '.*')\n",
    "    return corpus.raw()\n",
    "\n",
    "def vocavulario(tokens):\n",
    "\tvocabulary = sorted(set(tokens))\n",
    "\treturn vocabulary\n",
    "\n",
    "def encontrarContextos(voc, window):\n",
    "    contextos = dict()\n",
    "    id = 0\n",
    "\n",
    "    for id in range(len(voc)):\n",
    "        word = voc[id]\n",
    "\n",
    "        if word not in contextos:\n",
    "            contextos[word] = []\n",
    "        start = max(0, id - window)\n",
    "        end = min(id + window, len(voc) - 1)\n",
    "\n",
    "        for i in range(start, end + 1):\n",
    "            if i != id:\n",
    "                contextos[word].append(voc[i])\n",
    "\n",
    "    return contextos\n",
    "\n",
    "def removeP(text):\n",
    "\tgood = {'\\n'}\n",
    "\tfor i in \"abcdefghijklmnopqrstuvwxyz áéíóúñü\":\n",
    "\t\tgood.add(i)\n",
    "\tans = \"\"\n",
    "\tfor c in text:\n",
    "\t\tif c in good:\n",
    "\t\t\tans += c\n",
    "\treturn ans\n",
    "\n",
    "def limpiarHTML(html):\n",
    "\treturn BeautifulSoup(html,'html.parser').get_text().lower()\n",
    "\n",
    "def splitText(txt):\n",
    "\treturn txt.replace('/', ' ').replace('.', ' ').replace('-', ' ')\n",
    "\n",
    "def lamma_dict(path):\n",
    "    with open(path, encoding='latin-1') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    lemm = {}\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            words = line.split()\n",
    "            token = words[0].strip()\n",
    "            token = token.replace(\"#\", \"\")\n",
    "            lemma = words[-1].strip()\n",
    "            lemm[token] = lemma\n",
    "    return list(lemm.items())\n",
    "\n",
    "def lematizar(text, lemm_dir):\n",
    "    lemmatized = []\n",
    "    lemmas = dict(lamma_dict(lemm_dir))\n",
    "    for word in text:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized.append(word)\n",
    "    return lemmatized\n",
    "\n",
    "def removeTildes(text):\n",
    "\ta,b = 'áéíóúüñÁÉÍÓÚÜÑ','aeiouunAEIOUUN'\n",
    "\ttrans = str.maketrans(a,b)\n",
    "\treturn text.translate(trans)\n",
    "\n",
    "def removeStopWords(tokens, stop_words):\n",
    "    normalized_tokens = []\n",
    "\n",
    "    for w in tokens:  \n",
    "        if w not in stop_words:  \n",
    "            normalized_tokens.append(w)\n",
    "\n",
    "    return normalized_tokens\n",
    "\n",
    "def palabrasSimilares(con, voc):\n",
    "    vectors = dict()\n",
    "\n",
    "    for target, context in con.items():\n",
    "        vectors2 = dict()\n",
    "        \n",
    "        for word in context:\n",
    "            if word not in vectors2:\n",
    "                vectors2[word] = 0\n",
    "            vectors2[word] += 1\n",
    "\n",
    "        l = numpy.zeros(len(voc))\n",
    "\n",
    "        for i in range(len(voc)):\n",
    "            word = voc[i]\n",
    "            if word in vectors2:\n",
    "                l[i] = vectors2[word]\n",
    "\n",
    "        vectors[target] = l\n",
    "\n",
    "    palabra = \"luis\"\n",
    "\n",
    "    ans = []\n",
    "    v1 = vectors[palabra]\n",
    "\n",
    "    print(v1)\n",
    "\n",
    "    \n",
    "\n",
    "    ans.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ans\n",
    "\n",
    "def encontrarSimilares(voc, con):\n",
    "    vectors = dict()\n",
    "\n",
    "    for target, context in con.items():\n",
    "        \n",
    "        vectors2 = dict()\n",
    "        \n",
    "        for word in context:\n",
    "            if word not in vectors2:\n",
    "                vectors2[word] = 0\n",
    "            vectors2[word] += 1\n",
    "\n",
    "        l = numpy.zeros(len(voc))\n",
    "\n",
    "        for i in range(len(voc)):\n",
    "            word = voc[i]\n",
    "            if word in vectors2:\n",
    "                l[i] = vectors2[word]\n",
    "\n",
    "        vectors[target] = l\n",
    "\n",
    "    print(vectors)\n",
    "\n",
    "    palabra = \"poder\"\n",
    "\n",
    "    ans = []\n",
    "    v1 = vectors[palabra]\n",
    "\n",
    "    for word in voc:\n",
    "        v2 = vectors[word]\n",
    "        cosine = numpy.dot(v1, v2) / numpy.sqrt(numpy.sum(v1 ** 2)) / numpy.sqrt(numpy.sum(v2 ** 2))\n",
    "        ans.append((word, cosine))\n",
    "\n",
    "    ans.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ans\n",
    "\n",
    "def imprimirResultados(similares):\n",
    "    for pares in similares:\n",
    "        if pares[1] != 0.0:\n",
    "            print(pares)\n",
    "\n",
    "html_original = openFile(\"EXCELSIOR_100_files\") \n",
    "\n",
    "html_limpio = limpiarHTML(html_original)\n",
    "\n",
    "html_limpio = splitText(html_limpio)\n",
    "\n",
    "html_limpio = removeP(html_limpio)\n",
    "\n",
    "tokens = word_tokenize(html_limpio, \"spanish\")\n",
    "\n",
    "stop_words = stopwords.words(\"spanish\")\n",
    "\n",
    "normalized_tokens = []\n",
    "\n",
    "normalized_tokens = removeStopWords(tokens, stop_words)\n",
    "\n",
    "lemmatized_tokens = []\n",
    "\n",
    "lemmatized_tokens = lematizar(normalized_tokens, \"generate_lemma.txt\")\n",
    "\n",
    "voc = vocavulario(lemmatized_tokens)\n",
    "\n",
    "con = encontrarContextos(voc, 4)\n",
    "\n",
    "similares = encontrarSimilares(voc, con)\n",
    "\n",
    "imprimirResultados(similares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
